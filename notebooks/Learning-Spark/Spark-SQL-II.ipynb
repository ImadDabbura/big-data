{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Iterator, Tuple\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://imads-mbp.attlocal.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Chapter 5</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc7258d66a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName('Chapter 5')\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# User Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It extends the capability of Spark by defining our own functions in Python. They do not persist across SparkSessions, so they are only avaialble in the same session they were defined (registered). Spark will store them in metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = spark.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.createOrReplaceTempView('udf_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubed(s):\n",
    "    return s * s * s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.cubed(s)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will make it available to use (stored in sql metastore)\n",
    "spark.udf.register(name='cube', f=cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|cube(id)|\n",
      "+---+--------+\n",
      "|  0|       0|\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "|  9|     729|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT id, cube(id) FROM udf_test').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL (SQL, DataFrame, and Dataset) does not guarantee the order of execution of subexpressions. For example,\n",
    "```sql\n",
    "spark.sql(\"SELECT s FROM test1 WHERE s IS NOT NULL AND strlen(s) > 1\")\n",
    "```\n",
    "The above query does not guarantee that checking for NULL will be evaluated before the `strlen(s)`. Therefore, it is recommended that we do the following:\n",
    "- Make the __UDF__ itself null-aware and do null checking inside the UDF.\n",
    "- Use `IF or CASE WHEN` expressions to do the null check and invoke the UDF in a conditional branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use pandas_udf to use vectorized operations instead of operating row by row which is very slow. With Pandas_UDFs, Pandas uses Apache Arrow to transfer data and Pandas will work on the data. Once the data is in Apache Arrow format, there is no need to serialize/pickle data. To make a Python UDF a Pandas UDF, we need to decorate the function using `@pandas_udf` and use __type hints__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Series to Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take `pd.Series` as input and returns `pd.Series` as output with the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_udf('long')\n",
    "def squares(s: pd.Series) -> pd.Series:\n",
    "    return s * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    2\n",
       "3    3\n",
       "4    4\n",
       "5    5\n",
       "6    6\n",
       "7    7\n",
       "8    8\n",
       "9    9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.Series(range(10))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     2\n",
       "2     4\n",
       "3     6\n",
       "4     8\n",
       "5    10\n",
       "6    12\n",
       "7    14\n",
       "8    16\n",
       "9    18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squares(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|(id * 2)|\n",
      "+---+--------+\n",
      "|  0|       0|\n",
      "|  1|       2|\n",
      "|  2|       4|\n",
      "|  3|       6|\n",
      "|  4|       8|\n",
      "|  5|      10|\n",
      "|  6|      12|\n",
      "|  7|      14|\n",
      "|  8|      16|\n",
      "|  9|      18|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id', squares(F.col('id'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the UDFs on pandas locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     2\n",
       "2     4\n",
       "3     6\n",
       "4     8\n",
       "5    10\n",
       "6    12\n",
       "7    14\n",
       "8    16\n",
       "9    18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squares(pd.Series(range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Iterator of Series to Iterator of Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Python function\n",
    "    - Takes an iterator of batches instead of a single input batch as input.\n",
    "    - Returns an iterator of output batches instead of a single output batch.\n",
    "- The length of the entire output in the iterator should be the same as the length of the entire input.\n",
    "- The wrapped pandas UDF takes a single Spark column as an input.\n",
    "- Specify the Python type hint as Iterator[pandas.Series] -> Iterator[pandas.Series].\n",
    "\n",
    "It is useful in the case of loading a machine learning model file to apply inference to every input batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# When the UDF is called with the column,\n",
    "# the input to the underlying function is an iterator of pd.Series.\n",
    "@pandas_udf(\"long\")\n",
    "def plus_one(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for x in batch_iter:\n",
    "        print(type(x))\n",
    "        yield x + 1\n",
    "\n",
    "df.select(plus_one(F.col(\"x\"))).show()\n",
    "# df.select(F.col(\"x\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the UDF, you can initialize some state before processing batches.\n",
    "# Wrap your code with try/finally or use context managers to ensure\n",
    "# the release of resources at the end.\n",
    "y_bc = spark.sparkContext.broadcast(1)\n",
    "\n",
    "@pandas_udf(\"long\")\n",
    "def plus_y(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    y = y_bc.value  # initialize states\n",
    "    try:\n",
    "        for x in batch_iter:\n",
    "            yield x + y\n",
    "    finally:\n",
    "        pass  # release resources here, if any\n",
    "\n",
    "df.select(plus_y(F.col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Iterator of multiple Series to Iterator of multiple Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences with Iterator of Series to Iterator of Series are:\n",
    "- The underlying Python function takes an iterator of a tuple of pandas Series.\n",
    "- The wrapped pandas UDF takes multiple Spark columns as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "@pandas_udf(\"long\")\n",
    "def multiply_two_cols(\n",
    "        iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    for a, b in iterator:\n",
    "        yield a * b\n",
    "\n",
    "df.select(multiply_two_cols(\"x\", \"x\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Series to scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is typically used in aggregation from one or more pandas Series to scalar such as __mean__. The type of the scalar can be Python primitive type or Numpy. It loads all the data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  1| 2.0|\n",
      "|  2| 3.0|\n",
      "|  2| 5.0|\n",
      "|  2|10.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('double')\n",
    "def mean_udf(x: pd.Series) -> float:\n",
    "    return x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(mean_udf('v')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Higher Order Functions in DataFrames and Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are typically two solutions for the manipulation of complex data types:\n",
    "- Exploding the nested structure into individual rows, applying some function, and then re-creating the nested structure as noted in the code snippet below (see Option 1)\n",
    "- Building a User Defined Function (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- values: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n",
      "+---+---------+\n",
      "| id|   values|\n",
      "+---+---------+\n",
      "|  1|[1, 2, 3]|\n",
      "|  2|[2, 3, 4]|\n",
      "|  3|[3, 4, 5]|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an array dataset\n",
    "arrayData = [[1, (1, 2, 3)], [2, (2, 3, 4)], [3, (3, 4, 5)]]\n",
    "\n",
    "# Create schema\n",
    "from pyspark.sql.types import *\n",
    "arraySchema = (StructType([\n",
    "      StructField(\"id\", IntegerType(), True), \n",
    "      StructField(\"values\", ArrayType(IntegerType()), True)\n",
    "      ]))\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(spark.sparkContext.parallelize(arrayData), arraySchema)\n",
    "df.createOrReplaceTempView(\"table\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explode(values) creates a new row (with the id) for each element (value) within values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|    1|\n",
      "|  1|    2|\n",
      "|  1|    3|\n",
      "|  2|    2|\n",
      "|  2|    3|\n",
      "|  2|    4|\n",
      "|  3|    3|\n",
      "|  3|    4|\n",
      "|  3|    5|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT id, explode(values) AS value\n",
    "FROM table\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Option 1: Explode & Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|new_values|\n",
      "+---+----------+\n",
      "|  1| [2, 3, 4]|\n",
      "|  3| [4, 5, 6]|\n",
      "|  2| [3, 4, 5]|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT id, collect_list(value + 1) AS new_values\n",
    "FROM (SELECT id, explode(values) AS value\n",
    "    FROM table) AS A\n",
    "GROUP BY id\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of values may not be the same as their order in the original array due to shuffling. This option is very expensive because we use `GroupBy` and it requires shuffling and values can be very wide/long. `collect_list` may cause out of memory issues for larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Option 2: User Defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plus_one(values):\n",
    "    return [v + 1 for v in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.plus_one(values)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register('plus_one', plus_one, returnType=ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|new_values|\n",
      "+---+----------+\n",
      "|  1| [2, 3, 4]|\n",
      "|  2| [3, 4, 5]|\n",
      "|  3| [4, 5, 6]|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "'''\n",
    "SELECT id, plus_one(values) AS new_values\n",
    "FROM table\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major drawback of this approach is that it requires serialization/deserialization which may be expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Higher Order Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView(\"tC\")\n",
    "t_c.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## `transform`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes an array and a lambda function, applies the function to each element in the array and returns an array. It is the same as UDF but more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "'''\n",
    "SELECT celsius, transform(celsius, v -> ((v * 9) div 5) + 32) AS fahrenheit\n",
    "FROM tC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## `filter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns only the elements where the predicate is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|             celsius|fahrenheit|\n",
      "+--------------------+----------+\n",
      "|[35, 36, 32, 30, ...|  [40, 42]|\n",
      "|[31, 32, 34, 55, 56]|  [55, 56]|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "'''\n",
    "SELECT celsius, filter(celsius, v -> v > 38) AS fahrenheit\n",
    "FROM tC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## `exists`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns __True__ if the predicate is true for any element in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "'''\n",
    "SELECT celsius, exists(celsius, v -> v = 38) AS threshold\n",
    "FROM tC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## `reduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "'''\n",
    "SELECT celsius, reduce(\n",
    "                    celsius, \n",
    "                    0, \n",
    "                    (x, y) -> x + y,\n",
    "                    y -> (y div size(celsius) * 9 div 5) + 32) AS avg_fahrenheit\n",
    "FROM tC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames and Spark SQL Common Relational Operators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|      City|State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|   BC| Canada| YXX|\n",
      "|  Aberdeen|   SD|    USA| ABR|\n",
      "|   Abilene|   TX|    USA| ABI|\n",
      "|     Akron|   OH|    USA| CAK|\n",
      "|   Alamosa|   CO|    USA| ALS|\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_na = (spark\n",
    "               .read\n",
    "               .format('csv')\n",
    "               .option('header', 'true')\n",
    "               .option('inferSchema', 'true')\n",
    "               .option('sep', '\\t')\n",
    "               .load('data/airport-codes-na.txt'))\n",
    "airports_na.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_na.createOrReplaceTempView('airports_na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = '''\n",
    "`date` STRING, `delay` INT, `distance` INT, `origin` STRING, `destination` STRING\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departure_delays = (spark\n",
    "               .read\n",
    "               .format('csv')\n",
    "               .option('header', 'true')\n",
    "               .option('schema', schema)\n",
    "               .load('data/departuredelays.csv'))\n",
    "departure_delays.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- delay: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departure_delays.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "departure_delays = (departure_delays\n",
    "                    .withColumn('delay', F.expr('CAST(delay AS INT) AS delay'))\n",
    "                    .withColumn('distance', F.expr('CAST(distance AS INT) AS distance')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departure_delays.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "departure_delays.createOrReplaceTempView('departure_delays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo = (departure_delays\n",
    "       .filter(F.expr(\n",
    "           \"\"\"origin == 'SEA' and destination == 'SFO' and date like '01010%' and delay > 0\"\"\")\n",
    "              ))\n",
    "foo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the same as `UNION ALL` in SQL. To get the SQL-like `UNION`, use `distinct` after `union`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = departure_delays.union(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(bar\n",
    " .filter(F.expr(\n",
    "           \"\"\"origin == 'SEA' and destination == 'SFO' and date like '01010%' and delay > 0\"\"\")\n",
    "              )\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `UnionByName`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default join is __inner__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+-----+-------+----+\n",
      "|    date|delay|distance|origin|destination|   City|State|Country|IATA|\n",
      "+--------+-----+--------+------+-----------+-------+-----+-------+----+\n",
      "|01010710|   31|     590|   SEA|        SFO|Seattle|   WA|    USA| SEA|\n",
      "|01010955|  104|     590|   SEA|        SFO|Seattle|   WA|    USA| SEA|\n",
      "|01010730|    5|     590|   SEA|        SFO|Seattle|   WA|    USA| SEA|\n",
      "+--------+-----+--------+------+-----------+-------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.join(airports_na, foo.origin == airports_na.IATA).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------------+\n",
      "|origin|destination|total_delays|\n",
      "+------+-----------+------------+\n",
      "|   SFO|        LAX|       40798|\n",
      "|   SEA|        ORD|       10041|\n",
      "|   SEA|        JFK|        4667|\n",
      "|   SEA|        SFO|       22293|\n",
      "|   JFK|        DEN|        4315|\n",
      "|   JFK|        LAX|       35755|\n",
      "|   SFO|        JFK|       24100|\n",
      "|   JFK|        SEA|        7856|\n",
      "|   SEA|        LAX|        9359|\n",
      "|   SFO|        ATL|        5091|\n",
      "|   JFK|        SFO|       35619|\n",
      "|   SEA|        ATL|        4535|\n",
      "|   JFK|        ORD|        5608|\n",
      "|   SEA|        DEN|       13645|\n",
      "|   JFK|        ATL|       12141|\n",
      "|   SFO|        ORD|       27412|\n",
      "|   SFO|        DEN|       18688|\n",
      "|   SFO|        SEA|       17080|\n",
      "+------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS departure_delays_window\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE departure_delays_window AS\n",
    "SELECT origin, destination, sum(delay) as total_delays \n",
    "  FROM departure_delays \n",
    " WHERE origin IN ('SEA', 'SFO', 'JFK') \n",
    "   AND destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL') \n",
    " GROUP BY origin, destination\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"SELECT * FROM departure_delays_window\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------------+----+\n",
      "|origin|destination|total_delays|rank|\n",
      "+------+-----------+------------+----+\n",
      "|   SEA|        SFO|       22293|   1|\n",
      "|   SEA|        DEN|       13645|   2|\n",
      "|   SEA|        ORD|       10041|   3|\n",
      "|   SFO|        LAX|       40798|   1|\n",
      "|   SFO|        ORD|       27412|   2|\n",
      "|   SFO|        JFK|       24100|   3|\n",
      "|   JFK|        LAX|       35755|   1|\n",
      "|   JFK|        SFO|       35619|   2|\n",
      "|   JFK|        ATL|       12141|   3|\n",
      "+------+-----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT origin, destination, total_delays, rank\n",
    "FROM (\n",
    "    SELECT origin, destination, total_delays, dense_rank() OVER (PARTITION BY origin ORDER BY total_delays DESC) AS rank\n",
    "    FROM departure_delays_window\n",
    ") AS A\n",
    "WHERE rank <= 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Adding new columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|on_time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo2 = (foo\n",
    "        .withColumn('status', F.expr(\"CASE WHEN delay <= 10 THEN 'on_time' ELSE 'delayed' END\")))\n",
    "foo2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|delayed|\n",
      "|01010955|     590|   SEA|        SFO|delayed|\n",
      "|01010730|     590|   SEA|        SFO|on_time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo3 = foo2.drop('delay')\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      delayed|\n",
      "|01010955|     590|   SEA|        SFO|      delayed|\n",
      "|01010730|     590|   SEA|        SFO|      on_time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo4 = (foo3\n",
    "        .withColumnRenamed('status', 'flight_status'))\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------+-----+\n",
      "|destination|CAST(substring(date, 0, 2) AS INT)|delay|\n",
      "+-----------+----------------------------------+-----+\n",
      "|        ORD|                                 1|   92|\n",
      "|        JFK|                                 1|   -7|\n",
      "|        DFW|                                 1|   -5|\n",
      "|        MIA|                                 1|   -3|\n",
      "|        DFW|                                 1|   -3|\n",
      "|        DFW|                                 1|    1|\n",
      "|        ORD|                                 1|  -10|\n",
      "|        DFW|                                 1|   -6|\n",
      "|        DFW|                                 1|   -2|\n",
      "|        ORD|                                 1|   -3|\n",
      "|        ORD|                                 1|    0|\n",
      "|        DFW|                                 1|   23|\n",
      "|        DFW|                                 1|   36|\n",
      "|        ORD|                                 1|  298|\n",
      "|        JFK|                                 1|    4|\n",
      "|        DFW|                                 1|    0|\n",
      "|        MIA|                                 1|    2|\n",
      "|        DFW|                                 1|    0|\n",
      "|        DFW|                                 1|    0|\n",
      "|        ORD|                                 1|   83|\n",
      "+-----------+----------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS INT), delay\n",
    "FROM departure_delays\n",
    "WHERE origin = 'SEA'\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+-------------+-------------+\n",
      "|destination|Jan_avg_delay|Jan_max_delay|Feb_avg_delay|Feb_max_delay|\n",
      "+-----------+-------------+-------------+-------------+-------------+\n",
      "|        GEG|         2.28|           63|         2.87|           60|\n",
      "|        BUR|        -2.03|           56|        -1.89|           78|\n",
      "|        SNA|        -3.58|           82|        -0.41|           90|\n",
      "|        OAK|        15.82|          385|         8.12|          150|\n",
      "|        DCA|        -1.15|           50|         0.07|           34|\n",
      "|        KTN|         2.40|           64|         5.00|          128|\n",
      "|        LIH|        -1.64|           15|        -1.70|           21|\n",
      "|        IAH|        17.91|          227|         7.79|          466|\n",
      "|        HNL|         7.43|          294|         2.94|          132|\n",
      "|        SJC|         4.46|          256|         2.74|           74|\n",
      "|        CVG|        -0.50|            4|         null|         null|\n",
      "|        AUS|         3.48|           50|        -0.21|           18|\n",
      "|        LGB|         6.67|          320|         2.06|          365|\n",
      "|        RNO|        10.19|           65|        14.18|          176|\n",
      "|        JAC|         1.25|           27|        -7.75|           -6|\n",
      "|        BOS|         7.84|          110|        14.58|          152|\n",
      "|        EWR|         9.63|          236|         5.20|          212|\n",
      "|        LAS|         9.42|          247|         7.78|          610|\n",
      "|        FAI|         1.84|          160|         4.21|           60|\n",
      "|        DEN|        13.13|          425|        12.95|          625|\n",
      "+-----------+-------------+-------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT *\n",
    "FROM (\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS INT) AS month, delay\n",
    "FROM departure_delays\n",
    "WHERE origin = 'SEA'\n",
    ")\n",
    "PIVOT (\n",
    "    CAST(AVG(delay) AS DECIMAL(4, 2)) AS avg_delay, MAX(delay) AS max_delay\n",
    "    FOR month in (1 Jan, 2 Feb)\n",
    ")\n",
    "\"\"\"\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
