{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://imads-mbp.attlocal.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Chapter 4</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa428ca7820>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName('Chapter 4')\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# SQL Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create schema using DDL-like or StructType kinds of definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDL\n",
    "schema = '''\n",
    "`date` STRING, `delay` INT, `distance` INT, `origin` STRING, `destination` STRING\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark\n",
    "      .read\n",
    "      .format('csv')\n",
    "      .option('header', True)\n",
    "      .option('schema', schema)\n",
    "      .load('data/departuredelays.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|date    |delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|6    |602     |ABE   |ATL        |\n",
      "|01020600|-8   |369     |ABE   |DTW        |\n",
      "|01021245|-2   |602     |ABE   |ATL        |\n",
      "|01020605|-4   |602     |ABE   |ATL        |\n",
      "|01031245|-4   |602     |ABE   |ATL        |\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02/19  09:25'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because year is missing, this UDF will convert date into DateType\n",
    "def to_date_format_udf(d_str):\n",
    "    l = [char for char in d_str]\n",
    "    return \"\".join(l[0:2]) + \"/\" +  \"\".join(l[2:4]) + \" \" + \" \" +\"\".join(l[4:6]) + \":\" + \"\".join(l[6:])\n",
    "\n",
    "to_date_format_udf(\"02190925\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.to_date_format_udf(d_str)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register('to_date', to_date_format_udf, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|03131530|    0|    4330|   HNL|        JFK|\n",
      "|03071625|   -1|    4330|   HNL|        JFK|\n",
      "|03121530|   -3|    4330|   HNL|        JFK|\n",
      "|03021625|   14|    4330|   HNL|        JFK|\n",
      "|03061625|   -2|    4330|   HNL|        JFK|\n",
      "|03081530|    4|    4330|   HNL|        JFK|\n",
      "|03091530|   -7|    4330|   HNL|        JFK|\n",
      "|03011625|   -1|    4330|   HNL|        JFK|\n",
      "|03151530|    2|    4330|   HNL|        JFK|\n",
      "|03051625|   -6|    4330|   HNL|        JFK|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT * \n",
    "FROM us_delay_flights_tbl\n",
    "WHERE distance > 1000\n",
    "ORDER BY distance DESC\n",
    "''').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01090900|   -3|    4330|   JFK|        HNL|\n",
      "|01050900|   98|    4330|   JFK|        HNL|\n",
      "|01080900|   14|    4330|   JFK|        HNL|\n",
      "|01020900|    1|    4330|   JFK|        HNL|\n",
      "|01040900|  111|    4330|   JFK|        HNL|\n",
      "|01060900|   -2|    4330|   JFK|        HNL|\n",
      "|01070900|    3|    4330|   JFK|        HNL|\n",
      "|01010900|    6|    4330|   JFK|        HNL|\n",
      "|01110900|   -4|    4330|   JFK|        HNL|\n",
      "|01030900|  784|    4330|   JFK|        HNL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df\n",
    " .where('distance > 1000')\n",
    " .sort('distance', ascending=False)\n",
    " .show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|02131625|   -1|    4330|   HNL|        JFK|\n",
      "|02071625|   16|    4330|   HNL|        JFK|\n",
      "|02121625|  932|    4330|   HNL|        JFK|\n",
      "|02021625|   -5|    4330|   HNL|        JFK|\n",
      "|02061625|   -9|    4330|   HNL|        JFK|\n",
      "|02081625|   -1|    4330|   HNL|        JFK|\n",
      "|02091625|   -6|    4330|   HNL|        JFK|\n",
      "|02011625|   -1|    4330|   HNL|        JFK|\n",
      "|02151625|   -2|    4330|   HNL|        JFK|\n",
      "|02051625|   -8|    4330|   HNL|        JFK|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df\n",
    " .where('distance > 1000')\n",
    " .sort(F.desc('distance'))\n",
    " .show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|03201100|  160|    1604|   SFO|        ORD|\n",
      "|03311810|  139|    1604|   SFO|        ORD|\n",
      "|03311405|  196|    1604|   SFO|        ORD|\n",
      "|03120929|  143|    1604|   SFO|        ORD|\n",
      "|03141657|  165|    1604|   SFO|        ORD|\n",
      "|03171251|  151|    1604|   SFO|        ORD|\n",
      "|03171215|  189|    1604|   SFO|        ORD|\n",
      "|03260828|  184|    1604|   SFO|        ORD|\n",
      "|03261106|  173|    1604|   SFO|        ORD|\n",
      "|03272225|  160|    1604|   SFO|        ORD|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM us_delay_flights_tbl\n",
    "WHERE delay > 120\n",
    "AND origin = 'SFO'\n",
    "AND destination = 'ORD'\n",
    "ORDER BY distance DESC\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|02081410|  181|    1604|   SFO|        ORD|\n",
      "|02091800|  223|    1604|   SFO|        ORD|\n",
      "|02092330|  142|    1604|   SFO|        ORD|\n",
      "|02101800|  171|    1604|   SFO|        ORD|\n",
      "|02180925|  141|    1604|   SFO|        ORD|\n",
      "|02190925| 1638|    1604|   SFO|        ORD|\n",
      "|02271410|  145|    1604|   SFO|        ORD|\n",
      "|02071333|  182|    1604|   SFO|        ORD|\n",
      "|02081104|  137|    1604|   SFO|        ORD|\n",
      "|02091823|  156|    1604|   SFO|        ORD|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df\n",
    " .where((F.col('delay') > 120) & (F.col('origin') == 'SFO') & (F.col('destination') == 'ORD'))\n",
    " .sort('distance', ascending=False)\n",
    " .show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|03201100|  160|    1604|   SFO|        ORD|\n",
      "|03311810|  139|    1604|   SFO|        ORD|\n",
      "|03311405|  196|    1604|   SFO|        ORD|\n",
      "|03120929|  143|    1604|   SFO|        ORD|\n",
      "|03141657|  165|    1604|   SFO|        ORD|\n",
      "|03171251|  151|    1604|   SFO|        ORD|\n",
      "|03171215|  189|    1604|   SFO|        ORD|\n",
      "|03260828|  184|    1604|   SFO|        ORD|\n",
      "|03261106|  173|    1604|   SFO|        ORD|\n",
      "|03272225|  160|    1604|   SFO|        ORD|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df\n",
    " .where((F.col('delay') > 120))\n",
    " .where(F.col('origin') == 'SFO')\n",
    " .where(F.col('destination') == 'ORD')\n",
    " .sort('distance', ascending=False)\n",
    " .show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+----------------+\n",
      "|delay|origin|destination|Flight_Delays   |\n",
      "+-----+------+-----------+----------------+\n",
      "|92   |ABE   |ORD        |Short Delays    |\n",
      "|91   |ABE   |DTW        |Short Delays    |\n",
      "|9    |ABE   |ATL        |Tolerable Delays|\n",
      "|9    |ABE   |ATL        |Tolerable Delays|\n",
      "|9    |ABE   |ATL        |Tolerable Delays|\n",
      "|9    |ABE   |ORD        |Tolerable Delays|\n",
      "|9    |ABE   |ATL        |Tolerable Delays|\n",
      "|9    |ABE   |ATL        |Tolerable Delays|\n",
      "|89   |ABE   |DTW        |Short Delays    |\n",
      "|88   |ABE   |ATL        |Short Delays    |\n",
      "+-----+------+-----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT delay, origin, destination,\n",
    "       CASE\n",
    "          WHEN delay > 360 THEN 'Very Long Delays'\n",
    "          WHEN delay > 120 AND delay < 360 THEN  'Long Delays '\n",
    "          WHEN delay > 60 AND delay < 120 THEN  'Short Delays'\n",
    "          WHEN delay > 0 and delay < 60  THEN   'Tolerable Delays'\n",
    "          WHEN delay = 0 THEN 'No Delays'\n",
    "          ELSE 'No Delays'\n",
    "       END AS Flight_Delays\n",
    "FROM us_delay_flights_tbl\n",
    "ORDER BY origin, delay DESC\n",
    "\"\"\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+----------------+\n",
      "|    date|delay|distance|origin|destination|   Flight_Delays|\n",
      "+--------+-----+--------+------+-----------+----------------+\n",
      "|01290607|   92|     569|   ABE|        ORD|    Short Delays|\n",
      "|02050600|   91|     369|   ABE|        DTW|    Short Delays|\n",
      "|02091725|    9|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|03241725|    9|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|03090600|    9|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|01050605|    9|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|01211219|    9|     569|   ABE|        ORD|Tolerable Delays|\n",
      "|03271725|    9|     602|   ABE|        ATL|Tolerable Delays|\n",
      "|01210600|   89|     369|   ABE|        DTW|    Short Delays|\n",
      "|01051245|   88|     602|   ABE|        ATL|    Short Delays|\n",
      "+--------+-----+--------+------+-----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df\n",
    " .withColumn('Flight_Delays', F.when(F.col('delay') > 360, 'Very Long Delays')\n",
    "             .when((F.col('delay') < 360) & (F.col('delay') > 120), 'Long Delays')\n",
    "             .when((F.col('delay') < 120) & (F.col('delay') > 60), 'Short Delays')\n",
    "             .when((F.col('delay') < 60) & (F.col('delay') > 0), 'Tolerable Delays')\n",
    "             .otherwise('No Delays'))\n",
    " .sort(F.asc('origin'), F.desc('delay'))\n",
    " .show(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# SQL Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/Users/imad/Downloads/bigdata/notebooks/Learning-Spark/spark-warehouse'),\n",
       " Database(name='learn_spark_db', description='', locationUri='file:/Users/imad/Documents/courses/data-engineering/big-data/notebooks/Learning-Spark/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a database\n",
    "spark.sql('DROP DATABASE IF EXISTS learn_spark_db CASCADE')\n",
    "spark.sql('CREATE DATABASE learn_spark_db')\n",
    "spark.sql('USE learn_spark_db') # From this point, any commands we issue in our application to create tables \n",
    "                                # will result in the tables being created in this database and residing under \n",
    "                                # the database name learn_spark_db.\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create managed table\n",
    "spark.sql('CREATE TABLE us_delay_flights_tbl(data STRING, delat INT, distance INT, origing STRING, destination STRING)')\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create managed table using dataframe API\n",
    "(df\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .saveAsTable('us_delay_flights_tbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='departure_delays_window', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(dbName='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(dbName='learn_spark_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns('us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create External Tables (Unmanaged)\n",
    "spark.sql(\"DROP DATABASE IF EXISTS learn_spark_db CASCADE\")\n",
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")\n",
    "spark.sql('''\n",
    "CREATE TABLE us_delay_flights_tbl(data STRING, delat INT, distance INT, origing STRING, destination STRING)\n",
    "USING CSV OPTIONS (path 'data/departuredelays.csv')\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df\n",
    "#  .write\n",
    "#  .mode('overwrite')\n",
    "#  .option('path', 'data/departuredelays.csv') # That is where Spark determines whether to create managed/external tabel\n",
    "#  .saveAsTable('us_delay_flights_tbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/Users/imad/Downloads/bigdata/notebooks/Learning-Spark/spark-warehouse'),\n",
       " Database(name='learn_spark_db', description='', locationUri='file:/Users/imad/Documents/courses/data-engineering/big-data/notebooks/Learning-Spark/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have data sql tables, we can read them without the need to reading from files.\n",
    "```Python\n",
    "us_flights_df = spark.sql(\"SELECT * FROM us_delay_flights_tbl\") \n",
    "us_flights_df2 = spark.table(\"us_delay_flights_tbl\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# SQL Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to creating tables, Spark can create views on top of existing tables. Views can be global (visible across all SparkSessions on a given cluster) or session-scoped (visible only to a single SparkSession), and they are temporary: they disappear after your Spark application terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create temporary (global) views.\n",
    "```sql\n",
    "CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS \n",
    "SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE origin = 'SFO';\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n",
    "SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE origin = 'JFK'\n",
    "```\n",
    "You can accomplish the same thing with the DataFrame API as follows:\n",
    "```python\n",
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "    # Create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")\n",
    "```\n",
    "Once you’ve created these views, you can issue queries against them just as you would against a table. Keep in mind that when accessing a global temporary view you must use the prefix global_temp.<view_name>, because Spark creates global temporary views in a global temporary database called global_temp. For example:\n",
    "```sql\n",
    "SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\n",
    "```\n",
    "By contrast, you can access the normal temporary view without the global_temp prefix:\n",
    "```sql\n",
    "SELECT * FROM us_origin_airport_JFK_tmp_view\n",
    "```\n",
    "```python\n",
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\")\n",
    "// Or\n",
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\")\n",
    "```\n",
    "You can also drop a view just like you would a table:\n",
    "```sql\n",
    "DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view;\n",
    "DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view\n",
    "```\n",
    "```python\n",
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\") \n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The difference between temporary and global temporary views being subtle, it can be a source of mild confusion among developers new to Spark. A temporary view is tied to a single SparkSession within a Spark application. In contrast, a global temporary view is visible across multiple SparkSessions within a Spark application. Yes, you can create multiple SparkSessions within a single Spark application—this can be handy, for example, in cases where you want to access (and combine) data from two different SparkSessions that don’t share the same Hive metastore configurations.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patten to reading data from different data sources:\n",
    "```python\n",
    "DataFrameReader.format(args).option(\"key\", \"value\").schema(args).load()\n",
    "```\n",
    "Since we can only get a handle of DataFrameReader using `SparkSession.read`, therefore:\n",
    "```python\n",
    "spark.read.format(args).option(\"key\", \"value\").schema(args).load()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parquet__ is the recommended data format for Spark due to its efficiency in reading and space and it is a columnar storage --> Helps __Catalyst__ optimizer. Parquet saves the schema as metadata so we don't actually need to provide a schema. It is also the default format so we don't need to provide `format` argument to the DataFrameReader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`saveAsTable` will persist the data in Hive metastore. If we don't have Hive installed, Spark will create that for us and use the metastore going forward. This command with either create a managed table or external table (if path option is provided)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Writing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "(DataFrameWriter\n",
    " .format(args)\n",
    " .option(args)\n",
    " .bucketBy(args)\n",
    " .partitionBy(args)\n",
    " .save(path)\n",
    "// Or\n",
    "DataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to save data in Parquet format due to its I/O optimizations and compressions. Files will be stored in a directory structure that has the metadata (schema, version, path, etc), data files, compressed files, and some status files. We only need to specify schema when reading from stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.save('test-df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "CREATE OR REPLACE TEMP VIEW us_delay_flights\n",
    "USING parquet\n",
    "OPTIONS (path \"test-df.parquet\")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='us_delay_flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='us_delay_flights_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|02151800|  108|     290|   ORD|        MSP|\n",
      "|02151800|  142|     772|   ORD|        DEN|\n",
      "|02151303|   16|    1516|   ORD|        LAX|\n",
      "|02151157|    7|    1316|   ORD|        LAS|\n",
      "|02151818|   55|    1511|   ORD|        PDX|\n",
      "|02151033|   12|     873|   ORD|        MCO|\n",
      "|02150941|    0|    1499|   ORD|        SNA|\n",
      "|02151320|   17|    1604|   ORD|        SFO|\n",
      "|02151804|    2|    1497|   ORD|        SAN|\n",
      "|02152000|   17|     119|   ORD|        GRR|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM us_delay_flights').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is another popular format that comes in two modes: \n",
    "- single line: Each line denotes a single JSON object\n",
    "- Multiline: The entire mutliline denotes a single JSON object\n",
    "\n",
    "We can use the same methods to read and write JSON files but change the format into JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically the fields are separated by commas; however, we can have the fields separated by other delimiters.\n",
    "```sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl USING csv\n",
    "OPTIONS (\n",
    "    path \"path-to-csv-files/*\", \n",
    "    header \"true\",\n",
    "    inferSchema \"true\",\n",
    "    mode \"FAILFAST\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Other Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other formats supported by Spark such as ORC, Avro (used by Kafka), Images, and Binary files. They all follow the same patterns in terms of reading and writing data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
