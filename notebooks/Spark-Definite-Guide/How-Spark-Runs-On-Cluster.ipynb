{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://imads-mbp.attlocal.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>How-Spark-Runs-On-Cluster</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd2e8a796d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName('How-Spark-Runs-On-Cluster')\n",
    "         .getOrCreate())\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.range(2, 10000000, 2)\n",
    "df2 = spark.range(2, 10000000, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = df1.repartition(5)\n",
    "step12 = df2.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2 = step1.selectExpr(\"id * 5 as id\")\n",
    "step3 = step2.join(step12, [\"id\"])\n",
    "step4 = step3.selectExpr(\"sum(id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(id)=2500000000000)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step4.collect() # 2500000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(7) HashAggregate(keys=[], functions=[sum(id#8L)])\n",
      "+- Exchange SinglePartition, true, [id=#66]\n",
      "   +- *(6) HashAggregate(keys=[], functions=[partial_sum(id#8L)])\n",
      "      +- *(6) Project [id#8L]\n",
      "         +- *(6) SortMergeJoin [id#8L], [id#2L], Inner\n",
      "            :- *(3) Sort [id#8L ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(id#8L, 200), true, [id=#50]\n",
      "            :     +- *(2) Project [(id#0L * 5) AS id#8L]\n",
      "            :        +- Exchange RoundRobinPartitioning(5), false, [id=#46]\n",
      "            :           +- *(1) Range (2, 10000000, step=2, splits=8)\n",
      "            +- *(5) Sort [id#2L ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(id#2L, 200), true, [id=#57]\n",
      "                  +- Exchange RoundRobinPartitioning(6), false, [id=#56]\n",
      "                     +- *(4) Range (2, 10000000, step=4, splits=8)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step4.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we aggregate on each partition, we bring all those aggregations to a single partition before sending the final result to the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark runs the jobs sequentially unless we use threading to launch multiple jobs in parallel.\n",
    "- Each action kicks off one job\n",
    "    - The job is broken down into stages based on the number of times we need to shuffle the data. Spark will try to combine as much tasks as possible and execute them in parallel w/o the need to shuffle.\n",
    "        - The stage is broken down into tasks, which are the actual unit of work that the executors perform on each partition. Therefore, it is a combination of a block data and a set of transformations that will run on a single executor. The higher the number of partitions, the more tasks that can be run in parallel.\n",
    "- The value of `spark.sql.shuffle.partitions` should be set according to the number of cores in the cluster. The default is 200.\n",
    "- The number of partitions should be > number of executors by multiple factor to achieve max parallelism.\n",
    "- Spark tries to use __Pipelining__ whenever possible. This means it tries to combine as many tasks as possible into one stage and do all the computations at once without the need to write the intermediate results to memory/disk. For example, if we have `select`, `filter`, then `select` on a DataFrame, Spark will do three computations in one go without writing the intermediate results into memory/disk --> Very fast.\n",
    "- __Shuffle Persistence__: Spark makes executors write __shuffle files__ to their disk during execution stage. This would help Spark not rerun the whole shuffling process when something failed. It also allows Spark to use those shuffle files if new job is executed on the already shuffled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
